{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "import math\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the relative path to the data directory\n",
    "data_folder_path = \"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Data\"\n",
    "non_games_data_folder_path = \"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Non_Games_Data\"\n",
    "\n",
    "# List all files in the data folder\n",
    "file_list = os.listdir(data_folder_path)\n",
    "file_list_non_games = os.listdir(non_games_data_folder_path)\n",
    "\n",
    "# Use glob to filter specific file types\n",
    "csv_files = glob.glob(os.path.join(data_folder_path, \"*.csv\"))\n",
    "csv_files_non_games = glob.glob(os.path.join(non_games_data_folder_path, \"*.csv\"))\n",
    "\n",
    "# Read in the weekly game data and concat into one combined df\n",
    "#dfs = [pd.read_csv(file) for file in csv_files]\n",
    "#combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Read in the supplementary data\n",
    "games = pd.read_csv(csv_files_non_games[0])\n",
    "nfl_colors = pd.read_csv(csv_files_non_games[1])\n",
    "pff_scouting_data = pd.read_csv(csv_files_non_games[2])\n",
    "players = pd.read_csv(csv_files_non_games[3])\n",
    "plays = pd.read_csv(csv_files_non_games[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip if ModelingDF hasn't Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_df = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/clean_df.csv\")\n",
    "ClusterDF = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/ClusterDF.csv\")\n",
    "RoutesDf = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/RoutesDf.csv\")\n",
    "StuntDF = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/StuntDF.csv\")\n",
    "C_AdjustedDF = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/C_AdjustedDF.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelingDF = RoutesDf.copy()\n",
    "\n",
    "ModelingDF = ModelingDF[[\"gameId\", \"playId\", \"nflId\", \"jerseyNumber\", \"frameId\", \"pff_positionLinedUp\", \"team\", \"s\", \"a\", \"dir\", \"clean_dir\", \"clean_o\", \"x\", \"y\"]]\n",
    "\n",
    "ModelingDF = pd.merge(ModelingDF, ClusterDF, on=[\"gameId\", \"playId\"])\n",
    "\n",
    "ModelingDF = pd.merge(ModelingDF, C_AdjustedDF, on=[\"gameId\", \"playId\", \"nflId\", \"frameId\"])\n",
    "\n",
    "ModelingDF = pd.get_dummies(ModelingDF, columns=['pff_positionLinedUp'])\n",
    "ModelingDF.sort_values(by=[\"gameId\", \"playId\", \"nflId\", \"frameId\"], inplace=True)\n",
    "ModelingDF[\"Next_CAX\"] = ModelingDF.groupby([\"gameId\", \"playId\", \"nflId\"])[\"C_adjusted_x\"].shift(-1)\n",
    "\n",
    "ModelingDF[\"Next_CAY\"] = ModelingDF.groupby([\"gameId\", \"playId\", \"nflId\"])[\"C_adjusted_y\"].shift(-1)\n",
    "\n",
    "\n",
    "ModelingDF = ModelingDF.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids = list(ModelingDF['gameId'].unique())\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_index = random.sample(game_ids, int(16 * 0.8))\n",
    "\n",
    "\n",
    "\n",
    "train_x = ModelingDF[ModelingDF[\"gameId\"].apply(lambda x: x in train_index)].iloc[:, np.r_[6:8, 9:11, 13:224]]\n",
    "\n",
    "train_y_x = ModelingDF[ModelingDF[\"gameId\"].apply(lambda x: x in train_index)][\"Next_CAX\"].astype(int)\n",
    "train_y_y = ModelingDF[ModelingDF[\"gameId\"].apply(lambda x: x in train_index)][\"Next_CAY\"].astype(int)\n",
    "\n",
    "test_x = ModelingDF[ModelingDF[\"gameId\"].apply(lambda x: x not in train_index)].iloc[:, np.r_[6:8, 9:11, 13:224]]\n",
    "\n",
    "test_y = ModelingDF[ModelingDF[\"gameId\"].apply(lambda x: x not in train_index)][\"Next_CAY\"].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbooster = xgb.XGBRegressor()\n",
    "\n",
    "clf_x = GridSearchCV(xgbooster, {'max_depth': [1, 2, 4, 6, 8, 10],\n",
    "                            'n_estimators': [10, 25, 50, 100, 200, 500, 1000]}, verbose=1)\n",
    "clf_x.fit(train_x, train_y_x)\n",
    "\n",
    "\n",
    "clf_y = GridSearchCV(xgbooster, {'max_depth': [1, 2, 4, 6, 8, 10],\n",
    "                            'n_estimators': [10, 25, 50, 100, 200, 500, 1000]}, verbose=1)\n",
    "clf_y.fit(train_x, train_y_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Predictions back to modelingDF\n",
    "\n",
    "Model_Inputs = ModelingDF.iloc[:, np.r_[6:8, 9:11, 13:224]]\n",
    "\n",
    "ModelingDF[\"Model_x\"] = clf_x.predict(Model_Inputs)\n",
    "\n",
    "ModelingDF[\"Model_y\"] = clf_y.predict(Model_Inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelingDF.to_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/ModelingDF.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelingDF = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/ModelingDF.csv\")\n",
    "team_join = plays[[\"gameId\", \"playId\", \"possessionTeam\", \"defensiveTeam\"]]\n",
    "recivers = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Non_Games_Data/receiving_summary2021.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronoi Tesselations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "VOR_df = ModelingDF.copy()\n",
    "\n",
    "VOR_df = pd.merge(VOR_df, team_join, on = [\"gameId\", \"playId\"])\n",
    "\n",
    "VOR_df = VOR_df[(VOR_df[\"gameId\"] == 2021090900) & (VOR_df[\"playId\"] == 187) & (VOR_df[\"frameId\"] == 6) ]\n",
    "\n",
    "bounding_lowerL = {\"gameId\" : 2021090900, \"playId\" : 187, \"frameId\" : 6, \"x\" : VOR_df['x'].min() - 1, \"y\" : VOR_df['y'].min() - 1}\n",
    "bounding_upperL = {\"gameId\" : 2021090900, \"playId\" : 187, \"frameId\" : 6, \"x\" : VOR_df['x'].max() + 1, \"y\" : VOR_df['y'].max() + 1}\n",
    "bounding_lowerR = {\"gameId\" : 2021090900, \"playId\" : 187, \"frameId\" : 6, \"x\" : VOR_df['x'].min() - 1, \"y\" : VOR_df['y'].max() + 1}\n",
    "bounding_upperR = {\"gameId\" : 2021090900, \"playId\" : 187, \"frameId\" : 6, \"x\" : VOR_df['x'].max() + 1, \"y\" : VOR_df['y'].min() - 1}\n",
    "\n",
    "bounding_lowerL = pd.DataFrame([bounding_lowerL])\n",
    "bounding_upperL = pd.DataFrame([bounding_upperL])\n",
    "bounding_lowerR = pd.DataFrame([bounding_lowerR])\n",
    "bounding_upperR = pd.DataFrame([bounding_upperR])\n",
    "\n",
    "\n",
    "VOR_df = pd.concat([VOR_df, bounding_lowerL], ignore_index=True)\n",
    "VOR_df = pd.concat([VOR_df, bounding_upperL], ignore_index=True)\n",
    "VOR_df = pd.concat([VOR_df, bounding_lowerR], ignore_index=True)\n",
    "VOR_df = pd.concat([VOR_df, bounding_upperR], ignore_index=True)\n",
    "\n",
    "\n",
    "points = VOR_df[['x', 'y']].values\n",
    "\n",
    "# Create a Voronoi diagram\n",
    "vor = Voronoi(points)\n",
    "\n",
    "# Plot the Voronoi diagram\n",
    "fig, ax = plt.subplots()\n",
    "voronoi_plot_2d(vor, ax=ax)\n",
    "\n",
    "# Plot the original points if needed\n",
    "ax.plot(points[:, 0], points[:, 1], 'ko')\n",
    "\n",
    "# Customize the plot if desired\n",
    "plt.xlim(VOR_df['x'].min() - 1, VOR_df['x'].max() + 1)\n",
    "plt.ylim(VOR_df['y'].min() - 1, VOR_df['y'].max() + 1)\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarPlayers = [52430, 41282, 42489, 43454, 44881, 41233, 40011, 53434, 47834]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOR_df = ModelingDF.copy()\n",
    "\n",
    "VOR_df = pd.merge(VOR_df, team_join, on = [\"gameId\", \"playId\"])\n",
    "\n",
    "#VOR_df = VOR_df[(VOR_df[\"gameId\"] == 2021090900)]\n",
    "\n",
    "VOR_df['area'] = np.nan\n",
    "VOR_df['exp_area'] = np.nan\n",
    "\n",
    "VOR_df['Time'] = VOR_df.groupby(['gameId', 'playId'])['frameId'].transform(lambda x: x - x.min() + 1)\n",
    "VOR_df['WithinDown'] = VOR_df['Time'] / 25\n",
    "\n",
    "VOR_df.fillna(0, inplace=True)\n",
    "\n",
    "def calculate_adjusted_change(row):\n",
    "    if row['Time'] < 25:\n",
    "        return row['area_percentage_change'] / (math.log(row['Time'] + 1, 26))\n",
    "    else:\n",
    "        return row['area_percentage_change']\n",
    "\n",
    "\n",
    "\n",
    "def Calculate_Gravity(playerCoordinates = []):\n",
    "\n",
    "    TotalGravity = pd.DataFrame()\n",
    "\n",
    "    unique_frames = playerCoordinates[\"frameId\"].unique()\n",
    "\n",
    "\n",
    "    for frame in unique_frames:\n",
    "\n",
    "        frame_data = playerCoordinates[playerCoordinates[\"frameId\"] == frame]\n",
    "\n",
    "        bounding_lowerL = {'x' : frame_data['x'].min() - 1, 'y' : frame_data['y'].min() - 1, \n",
    "                           'Average_Cluster_CAX' : frame_data['Model_x'].min() - 1, 'Average_Cluster_CAY' : frame_data['Model_y'].min() - 1}\n",
    "        bounding_upperL = {'x' : frame_data['x'].max() + 1, 'y' : frame_data['y'].max() + 1,\n",
    "                           'Average_Cluster_CAX' : frame_data['Model_x'].max() + 1, 'Average_Cluster_CAY' : frame_data['Model_y'].max() + 1}\n",
    "        bounding_lowerR = {'x' : frame_data['x'].min() - 1, 'y' : frame_data['y'].max() + 1,\n",
    "                           'Average_Cluster_CAX' : frame_data['Model_x'].min() - 1, 'Average_Cluster_CAY' : frame_data['Model_y'].max() + 1}\n",
    "        bounding_upperR = {'x' : frame_data['x'].max() + 1, 'y' : frame_data['y'].min() - 1,\n",
    "                           'Average_Cluster_CAX' : frame_data['Model_x'].max() + 1, 'Average_Cluster_CAY' : frame_data['Model_y'].min() - 1}\n",
    "\n",
    "        bounding_lowerL = pd.DataFrame([bounding_lowerL])\n",
    "        bounding_upperL = pd.DataFrame([bounding_upperL])\n",
    "        bounding_lowerR = pd.DataFrame([bounding_lowerR])\n",
    "        bounding_upperR = pd.DataFrame([bounding_upperR])\n",
    "\n",
    "\n",
    "        frame_data = pd.concat([frame_data, bounding_lowerL], ignore_index=True)\n",
    "        frame_data = pd.concat([frame_data, bounding_upperL], ignore_index=True)\n",
    "        frame_data = pd.concat([frame_data, bounding_lowerR], ignore_index=True)\n",
    "        frame_data = pd.concat([frame_data, bounding_upperR], ignore_index=True)\n",
    "\n",
    "\n",
    "        points = frame_data[['x', 'y']].dropna().values\n",
    "\n",
    "        # Create a Voronoi diagram\n",
    "        vor = Voronoi(points)\n",
    "\n",
    "        # Iterate through the input points\n",
    "        for i, point in enumerate(points):\n",
    "            # Find the Voronoi region index for the current point\n",
    "            region_index = vor.point_region[i]\n",
    "            \n",
    "            # Get the vertices of the region\n",
    "            region_vertices = vor.regions[region_index]\n",
    "            \n",
    "            # Filter out invalid vertices\n",
    "            region_vertices = [vertex for vertex in region_vertices if vertex != -1]\n",
    "            \n",
    "            if len(region_vertices) > 0:\n",
    "                # Get the vertices of the region\n",
    "                vertices = vor.vertices[region_vertices]\n",
    "                \n",
    "                # Calculate the area using the Shoelace formula\n",
    "                area = 0.5 * np.abs(np.dot(vertices[:, 0], np.roll(vertices[:, 1], 1)) -\n",
    "                                np.dot(vertices[:, 1], np.roll(vertices[:, 0], 1)))\n",
    "                \n",
    "                # Store the area in the 'area' column of VOR_df for the current point\n",
    "                frame_data.at[i, 'area'] = area\n",
    "\n",
    "        ''' DO IT AGAIN FOR EXPECTED'''\n",
    "\n",
    "        exp_points = frame_data[['Average_Cluster_CAX', 'Average_Cluster_CAY']].dropna().values\n",
    "\n",
    "        # Create a Voronoi diagram\n",
    "        exp_vor = Voronoi(exp_points)\n",
    "\n",
    "        # Iterate through the input points\n",
    "        for i, point in enumerate(exp_points):\n",
    "            # Find the exp_voronoi region index for the current point\n",
    "            exp_region_index = exp_vor.point_region[i]\n",
    "            \n",
    "            # Get the vertices of the region\n",
    "            exp_region_verticies = exp_vor.regions[exp_region_index]\n",
    "            \n",
    "            # Filter out invalid vertices\n",
    "            exp_region_verticies = [vertex for vertex in exp_region_verticies if vertex != -1]\n",
    "            \n",
    "            if len(exp_region_verticies) > 0:\n",
    "                # Get the vertices of the region\n",
    "                exp_verticies = exp_vor.vertices[exp_region_verticies]\n",
    "                \n",
    "                # Calculate the area using the Shoelace formula\n",
    "                exp_area = 0.5 * np.abs(np.dot(exp_verticies[:, 0], np.roll(exp_verticies[:, 1], 1)) -\n",
    "                                np.dot(exp_verticies[:, 1], np.roll(exp_verticies[:, 0], 1)))\n",
    "                \n",
    "                # Store the area in the 'area' column of VOR_df for the current point\n",
    "                frame_data.at[i, 'exp_area'] = exp_area\n",
    "\n",
    "        TotalGravity = pd.concat([TotalGravity, frame_data], ignore_index=True) \n",
    "\n",
    "        TotalGravity.dropna(subset = [\"gameId\"], inplace = True)\n",
    "\n",
    "        TotalGravity['area_diff'] = TotalGravity['exp_area'] - TotalGravity['area']\n",
    "\n",
    "\n",
    "        # Sort the DataFrame by the grouping columns and frameId in ascending order\n",
    "        TotalGravity.sort_values(by=['gameId', 'playId', 'nflId', 'frameId'], inplace=True)\n",
    "\n",
    "        # Calculate the percentage change in the 'area' column within each group\n",
    "        TotalGravity['area_percentage_change'] = TotalGravity.groupby(['gameId', 'playId', 'nflId'])['area_diff'].diff()\n",
    "\n",
    "        TotalGravity['area_percentage_change'].fillna(0, inplace=True)\n",
    "\n",
    "        # Create the 'adjusted_change' column by applying the custom function to each row\n",
    "        TotalGravity['adjusted_change'] = TotalGravity.apply(calculate_adjusted_change, axis=1)\n",
    "\n",
    "        # Calculate the rolling average for 'adjusted_change' when 'Time' is greater than or equal to 25\n",
    "        TotalGravity['adjusted_change'] = TotalGravity['adjusted_change'].where(TotalGravity['Time'] < 25, TotalGravity['adjusted_change'].rolling(5, min_periods=1).mean())\n",
    "\n",
    "        TotalGravity[\"Play_Gravity\"] = TotalGravity.groupby([\"gameId\", \"playId\", \"nflId\"])[\"adjusted_change\"].transform(\"sum\")\n",
    "        TotalGravity[\"Game_Gravity\"] = TotalGravity.groupby([\"gameId\", \"nflId\"])[\"adjusted_change\"].transform(\"sum\")\n",
    "        TotalGravity[\"Total_Gravity\"] = TotalGravity.groupby([\"nflId\"])[\"adjusted_change\"].transform(\"sum\")\n",
    "        \n",
    "    return(TotalGravity)\n",
    "\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = [VOR_df[i:i + chunk_size] for i in range(0, len(VOR_df), chunk_size)]\n",
    "\n",
    "del ModelingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/Voronoi Chunks/\"\n",
    "i = 0\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Your code to process the chunk\n",
    "    processed_chunk = Calculate_Gravity(chunk)\n",
    "\n",
    "    data_name = f\"{folder_path}VoronoiChunk{i}.csv\"\n",
    "    \n",
    "    # Save the processed chunk to a specific location\n",
    "    processed_chunk.to_csv(data_name, index=False)\n",
    "\n",
    "    print(i)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOR_df.to_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/AllGamesRoutesVoronoi.csv\", index=False)\n",
    "#VOR_df = pd.read_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/AllGamesRoutesVoronoi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the data folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Use glob to filter specific file types\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Read in the weekly game data and concat into one combined df\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "VOR_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = stats.iqr(VOR_df[\"Play_Gravity\"])\n",
    "# Calculate the first quartile (Q1)\n",
    "Q1 = np.percentile(VOR_df[\"Play_Gravity\"], 25)\n",
    "\n",
    "# Calculate the third quartile (Q3)\n",
    "Q3 = np.percentile(VOR_df[\"Play_Gravity\"], 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_Play_Count = 75\n",
    "\n",
    "\n",
    "ExtraWork = VOR_df.copy()\n",
    "\n",
    "man_zone = plays[[\"gameId\", \"playId\", \"pff_passCoverageType\"]]\n",
    "ExtraWork = pd.merge(ExtraWork, man_zone, on = [\"gameId\", \"playId\"])\n",
    "\n",
    "ExtraWork = ExtraWork[ExtraWork[\"team\"] == ExtraWork[\"possessionTeam\"]]\n",
    "\n",
    "ExtraWork = ExtraWork.groupby('nflId').apply(lambda x: x[x['Play_Gravity'] <= Q3 + 1.5 * IQR]).reset_index(drop=True)\n",
    "ExtraWork = ExtraWork.groupby('nflId').apply(lambda x: x[x['Play_Gravity'] >= Q1 - 1.5 * IQR]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Group by 'nflId' and count the unique 'frameId' values in each group\n",
    "play_counts = ExtraWork.groupby('nflId')['playId'].nunique()\n",
    "\n",
    "# Create a new column 'TotalPlays' in the original DataFrame\n",
    "ExtraWork['TotalPlays'] = ExtraWork['nflId'].map(play_counts)\n",
    "ExtraWork = ExtraWork[ExtraWork[\"TotalPlays\"] >= Min_Play_Count]\n",
    "TotalPlays = ExtraWork[[\"nflId\", \"team\", \"TotalPlays\"]]\n",
    "\n",
    "Summarized = pd.DataFrame()\n",
    "Summarized[\"MeanFrame\"] = ExtraWork.groupby([\"nflId\"])[\"adjusted_change\"].mean()\n",
    "Summarized[\"MeanPlay\"] = ExtraWork.groupby([\"nflId\"])[\"Play_Gravity\"].mean()\n",
    "Summarized = Summarized.drop_duplicates()\n",
    "Summarized.reset_index(inplace=True)\n",
    "Summarized[\"StarPlayer\"] = np.where(Summarized[\"nflId\"].isin(StarPlayers), 1, 0)\n",
    "Summarized = Summarized.sort_values(by=['MeanPlay'], ascending = True)\n",
    "\n",
    "player_name = players[[\"nflId\", \"officialPosition\", \"displayName\"]]\n",
    "\n",
    "Summarized = pd.merge(Summarized, player_name, on = \"nflId\")\n",
    "Summarized = pd.merge(Summarized, TotalPlays, on = \"nflId\")\n",
    "\n",
    "\n",
    "Summarized = Summarized.drop_duplicates()\n",
    "\n",
    "\n",
    "recivers[\"Cluster\"] = np.where((recivers[\"avg_depth_of_target\"] >= 10) & (recivers[\"slot_rate\"] <= 49), 1, \n",
    "                               np.where((recivers[\"avg_depth_of_target\"] < 10) & (recivers[\"slot_rate\"] <= 49), 0.5,\n",
    "                                        np.where((recivers[\"avg_depth_of_target\"] >= 10) & (recivers[\"slot_rate\"] > 49), 0.25,\n",
    "                               0)))\n",
    "\n",
    "ReceiverClusters = recivers[[\"nflId\", \"avg_depth_of_target\", \"slot_rate\", \"Cluster\"]]\n",
    "\n",
    "Summarized = pd.merge(Summarized, ReceiverClusters, on = \"nflId\")\n",
    "\n",
    "\n",
    "Viewer = ExtraWork[['gameId', 'playId', 'nflId', 'jerseyNumber', \"team\", 'pff_passCoverageType', 'frameId', 'area', 'area_percentage_change', 'adjusted_change', 'Play_Gravity', 'Game_Gravity', \"Total_Gravity\"]].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalTeamByPlay = Viewer.groupby(['gameId', 'playId', \"team\"])[\"Play_Gravity\"].mean().reset_index()\n",
    "\n",
    "\n",
    "# Group by 'nflId' and count the unique 'frameId' values in each group\n",
    "play_counts_split = ExtraWork.groupby(['nflId', 'pff_passCoverageType'])['playId'].nunique().reset_index()\n",
    "\n",
    "\n",
    "PlayerManZoneSplit = Viewer.groupby(['nflId', 'pff_passCoverageType'])[\"Play_Gravity\"].mean().reset_index()\n",
    "#PlayerManZoneSplit = PlayerManZoneSplit[PlayerManZoneSplit[\"nflId\"].isin(StarPlayers)]\n",
    "PlayerManZoneSplit = pd.merge(PlayerManZoneSplit, player_name, on = \"nflId\")\n",
    "PlayerManZoneSplit = pd.merge(PlayerManZoneSplit, play_counts_split, on = ['nflId', 'pff_passCoverageType'])\n",
    "PlayerManZoneSplit = PlayerManZoneSplit[PlayerManZoneSplit[\"playId\"] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarized.to_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/AllGamesRoutesSummarized.csv\", index=False)\n",
    "#PlayerManZoneSplit.to_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/AllGamesRoutesPlayerManZoneSplit.csv\", index=False)\n",
    "#Viewer.to_csv(\"C:/Users/sethl/OneDrive/Important Stuff/R/R files/NFL/DataBowl/2024-Big-Data-Bowl/Created_DF/AllGamesRoutesViewer.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S52",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
